---
title: "NYSAT"
author: "EKLAR"
date: 'null'
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Links to the Datasource

+ NYC sat scores for 2012-https://data.cityofnewyork.us/Education/2012-SAT-Results/f9bf-2cp4

+ school accountability- https://data.cityofnewyork.us/Education/2006-2012-School-Demographics-and-Accountability-S/ihfw-zy9j

+ NYC general education survey-https://data.cityofnewyork.us/Education/2012-NYC-General-Education-School-Survey/xiyj-m4sj


```{r,include=FALSE}
#Loading Required Library's
# install these library's if they are missing in your local machine
#with install.packages()
#install.packages('here')
#install.packages('data.table')
#install.packages('dplyr')
#install.packages("corrplot", dependencies = TRUE)
#install.packages("ztable")
#install.packages("moments")

library(here) 
library(data.table)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(corrplot)
library(ztable)
library(moments)
library(readxl)
library(openxlsx)
```

```{r, reading the data,include=FALSE}
sat_scores<-
  fread(here("RawData","2012_SAT_Results.csv"))
school_demographic<-
  fread(here("RawData","2006_-_2012_School_Demographics_and_Accountability_Snapshot.csv"))
school_survey<-
  fread(here("RawData","2012_NYC_General_Education_School_Survey.csv"))
```

```{r,include=FALSE}
# Joining the three datasets
school_demographic2012<-school_demographic%>%filter(schoolyear=="20112012")
joined_satanddemographic<-left_join(sat_scores,school_demographic2012,"DBN")
all_joined<-left_join(joined_satanddemographic,school_survey,"DBN")

df<- 
  all_joined%>%select(DBN,
                      `SCHOOL NAME`,
                      'School Type',
                      `Num of SAT Test Takers`,
                      'total_enrollment',
                      `SAT Critical Reading Avg. Score`,
                      `SAT Math Avg. Score`,
                      `SAT Writing Avg. Score`,
                      male_per,
                      female_per,
                      black_per,
                      white_per,
                      hispanic_per,
                      asian_per,
                      ell_percent,
                      frl_percent,
                      `Total Engagement Score`) 

df$`Num of SAT Test Takers`[] <- 
  lapply(df$`Num of SAT Test Takers`,function(x)as.character(gsub("s", NA, x))) #replacing "s" with NA's

df<-na.omit(df)
df<-df%>%filter(!is.na(df$`Num of SAT Test Takers`))


df[4:length(df)]<-lapply(df[4:length(df)],function(x) as.numeric(x)) # converting 

str(df) # checking to makes sure that the all the character columns have been 
        # converted to numeric

```

```{r, finding averages by school categories,include=FALSE}
 Readingaverage =  tapply(df$`SAT Critical Reading Avg. Score`,df$`School Type`, mean)
 Mathaverage =  tapply(df$`SAT Math Avg. Score`,df$`School Type`, mean)
 Writingaverage =  tapply(df$`SAT Writing Avg. Score`,df$`School Type`, mean)
 ReadingSD =  tapply(df$`SAT Critical Reading Avg. Score`,df$`School Type`, sd)
 MathSD =  tapply(df$`SAT Math Avg. Score`,df$`School Type`, sd)
 WritingSD =  tapply(df$`SAT Writing Avg. Score`,df$`School Type`, sd)

 
 SATtesttaker =  tapply(df$`Num of SAT Test Takers`,df$`School Type`, sum)
 TotalEnrollment =  tapply(df$total_enrollment,df$`School Type`, sum)
Read =c(Readingaverage)
Math =c(Mathaverage)
Writing = c(Writingaverage)
ReadSD = c(ReadingSD)
MathsSD = c(MathSD) 
WritSD =c(WritingSD)
Taker= c(SATtesttaker)
Enrol= c (TotalEnrollment)
rbind(Read, Math, Writing, ReadSD, MathsSD, WritSD, Taker, Enrol)

# Can you also add the Number of Schools (n) for each school type
```




```{r}
#include summary of dataset 
summary(df)   
```

#Explorotary data analysis

```{r, explore data}
# How SAT is scored-(Ehtesham)

# check the code in the Joining the three datasets section to make sure that 
 # we have the nessesary variables + any accuracy issues (Ehtesham)

# add the survey response columns from all_joined to df dataframe (laknath)

# make characrter columns numeric that have SAT scores and percentage (Abinav)

# Based on the real data deternime how we combine scores/or whether to keep seperate (Abhinav)
  # should include histogram also boxplot (abhinav)

# see if there is difference in aveage scores between just high scool vs those other types
  # see if tere is difference between male and female (Ehtesham)


# correlation plot (multi coleniarity) -Raj
    # also include the numbers -Raj

# plot histogram of (distribution) of SAT scores -Raj/Kushboo

```  




# Determine if there is a relationship between enrollment and SAT scores-scatterplot (Laknath)
```{r}
pl_math<-ggplot(data = df)+
  geom_point(mapping = aes(x = `Num of SAT Test Takers`,y = `SAT Math Avg. Score`),
             color="red",position = "jitter")

pl_reading<-ggplot(data = df)+
  geom_point(mapping = aes(x =`Num of SAT Test Takers`,y =`SAT Critical Reading Avg. Score`),color="green",position = "jitter")

pl_writing<-ggplot(data = df)+
  geom_point(mapping = aes(x =`Num of SAT Test Takers` ,y = `SAT Writing Avg. Score`),
             color="orange",position = "jitter")

scores_sattakers<-ggarrange(pl_math,pl_reading,pl_writing)

ggsave(filename = file.path(here("plots","scoresandtakers.png")))
```
It looks like there is a positve linear association between enrollment and SAT reading scores

```{r, RAJ}
#Error check
df$totgen = df$male_per + df$female_per
df$totdiv = df$black_per + df$white_per + df$hispanic_per + df$asian_per

#Simpson Diversity index
df$div = 1 - (((df$black_per*(df$black_per-1))+(df$white_per*(df$white_per-1))+(df$hispanic_per*(df$hispanic_per-1))+(df$asian_per*(df$asian_per-1)))/9900)

# Missing data check
#summary(dfr)
#apply(dfr,2,function(x) sum(is.na(x)))
#apply(dfr,2,function(x) sum(is.na(x))*100/nrow(dfr)) #percentage of NAs

percentmiss = function(x){sum(is.na(x))/length(x)*100} #percent miss row
missing = apply(df,1,percentmiss)
table(missing)
replace = subset(df, missing <= 5)
missing1 = apply(replace,1,percentmiss)
table(missing1)
dont = subset(df, missing > 5)
missing2 = apply(dont,1,percentmiss)
table(missing2)
#apply(replace,2,percentmiss) #missing in column


# Outlier check
mah = mahalanobis(replace[,-c(1,2,3,18,19,20)],
                    colMeans(replace[,-c(1,2,3,18,19,20)], na.rm=TRUE),
                    cov(replace[,-c(1,2,3,18,19,20)], use ="pairwise.complete.obs")
                    )    # This piece of code doesn't seem to work
#mah
cutoff = qchisq(1-.001,ncol(replace))
#print(cutoff)
summary(mah < cutoff)
noout = subset(replace, mah < cutoff) #Eliminate outliers
#str(noout)
```
# Correlation
```{r results='asis'}
# Additivity check
corrplot(cor(noout[,-c(1,2,3,18,19)]))
cormat = cor(noout[,-c(1,2,3,18,19)]) #Correlation quantified

cormat %>%
  as.data.frame() %>%
  ztable() %>% 
  makeHeatmap(mycolor = gradientColor(low="red",mid="white",high="blue")) %>%
  print(caption="Correlation Heatmap")
```

```{r, Norm}
#Normality check
plot.new()
par(mfrow=c(3, 3)); hist(noout$`SAT Critical Reading Avg. Score`, breaks=15, main = "SAT Reading", xlab=NA, ylab=NA); hist(noout$`SAT Math Avg. Score`, breaks=15, main = "SAT Math", xlab=NA, ylab=NA); hist(noout$`SAT Writing Avg. Score`, breaks=15, main = "SAT Writing", xlab=NA, ylab=NA); hist(noout$ell_percent, breaks=15, main = "ELL", xlab=NA, ylab=NA); hist(noout$frl_percent, breaks=15, main = "FRL", xlab=NA, ylab=NA); hist(noout$female_per, breaks=15, main = "Female %", xlab=NA, ylab=NA); hist(noout$white_per, breaks=15, main = "White %", xlab=NA, ylab=NA); hist(noout$black_per, breaks=15, main = "Black %", xlab=NA, ylab=NA); hist(noout$asian_per, breaks=15, main = "Asian %", xlab=NA, ylab=NA)

#apply(noout[,-c(1,2,16,17)], 2, skewness, na.rm =TRUE)
#apply(noout[,-c(1,2,16,17)], 2, kurtosis, na.rm =TRUE)

hist(noout$div, breaks=25, main = "Diversity", xlab=NA, ylab=NA)

par(mfrow=c(2, 2)); plot(noout$white_per, noout$asian_per); plot(noout$white_per, noout$black_per); plot(noout$hispanic_per, noout$black_per); plot(noout$hispanic_per, noout$asian_per)

plot(noout$white_per, noout$`SAT Writing Avg. Score`)

```

```{r, Laknath Code Section}
#https://nces.ed.gov/programs/coe/pdf/coe_clb.pdf

df_poverty<-df%>%mutate(poverty=case_when(frl_percent>50~"mid to high poverty",
                              frl_percent<=50~"mid low to low poverty"))
poverty_low<-df_poverty%>%filter(poverty=="mid to high poverty")
hist(poverty_low$`SAT Math Avg. Score`)
poverty_high<-df_poverty%>%filter(poverty=="mid low to low poverty")
hist(poverty_high$`SAT Math Avg. Score`)

df_poverty<-df_poverty%>%
  filter(poverty %in% c("mid to high poverty","mid low to low poverty"))
  
povertyplot<-ggplot(data = df_poverty,aes(x = frl_percent,y = `SAT Math Avg. Score`))+
  geom_point(aes(color=poverty))+
  xlab("Percentage of students with free or reduced lunches")+
  ylab("Average SAT math score")+
  ggtitle("understanding relationship between poverty and SAT performance")

ggsave(filename = file.path(here("plots","povertyplot.png")))

t.test(`SAT Math Avg. Score` ~ poverty, data = df_poverty, var.equal = FALSE)

```


```{r, Raj Code Section}





















```
```{r, Kushboo Code Section}





















```
```{r, Abinav Code Section}





















```
```{r, Ehtesham Code Section}
head(df)
summary(df) # To see quartiles and mean
dist = table(df$`SAT Math Avg. Score`) # making a frequency table to see if its normal
barplot(dist) + cleanup
hist(dist, breaks=10) + cleanup
skewness(df$`SAT Math Avg. Score`) # 1.815097 since its greater than 1 therefore highly skewed
kurtosis(df$`SAT Math Avg. Score`) # 4.097635 High Kurtosis means the tails are longer and fatter and central peak is higher and sharper
 #1: Your dependent variable should be measured on a continuous scale.True
 #2: You have two or more independent variables. True
 #3: You should have independence of observations. True
 #4: There needs to be a linear relationship. Checked via scatter plots and line of best fit
#A Total Enrollment Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$total_enrollment, df$`SAT Math Avg. Score`,xlab = "Total Enrollment", ylab = "SAT Math AQvg Score") + cleanup
TEregA = lm(df$`SAT Math Avg. Score` ~ df$total_enrollment, data = df)
abline(TEregA,col="red")
distA = table(df$total_enrollment) # making a frequency table to see if its normal
barplot(distA)
hist(distA, breaks = 10)
skewness(df$total_enrollment) # 2.756 since its greater than 1 therefore highly skewed
kurtosis(df$total_enrollment) # 7.43 High Kurtosis means the tails are longer and fatter and central peak is higher and sharper
#B Male Percent Scatter Plot  + Bar Plot + Skewness + Kutosis
plot(df$male_per, df$`SAT Math Avg. Score`,xlab = "Male %", ylab = "SAT Math AQvg Score")
TEregB = lm(df$`SAT Math Avg. Score` ~ df$male_per, data = df)
abline(TEregB,col="red")
distB = table(df$male_per) # making a frequency table to see if its normal
barplot(distB) 
hist(distB, breaks = 10)
skewness(df$male_per) # 0.1498 Normal skewness
kurtosis(df$male_per) # 3.58 High Kurtosis means the tails are longer and fatter and central peak is higher and sharper
#C Female Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$female_per, df$`SAT Math Avg. Score`,xlab = "Female %", ylab = "SAT Math AQvg Score")
TEregC = lm(df$`SAT Math Avg. Score` ~ df$female_per, data = df)
abline(TEregC,col="red")
distC = table(df$female_per) # making a frequency table to see if its normal
barplot(distC) 
hist(distC, breaks = 10)
skewness(df$female_per) # -0.1499 Normal skewness
kurtosis(df$female_per) # 3.58 Normal skewness
#D Black Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$black_per, df$`SAT Math Avg. Score`,xlab = "Black %", ylab = "SAT Math AQvg Score")
TEregD = lm(df$`SAT Math Avg. Score` ~ df$black_per, data = df)
abline(TEregD,col="red")
distD = table(df$black_per) # making a frequency table to see if its normal
barplot(distD) 
hist(distD, breaks = 10)
skewness(df$black_per) # 0.6190 Normal skewness
kurtosis(df$black_per) # -0.7412 Normal skewness
#E White Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$white_per, df$`SAT Math Avg. Score`,xlab = "WWhite %", ylab = "SAT Math AQvg Score")
TEregE = lm(df$`SAT Math Avg. Score` ~ df$white_per, data = df)
abline(TEregE,col="red")
distE = table(df$white_per) # making a frequency table to see if its normal
barplot(distE) 
hist(distE, breaks = 10)
skewness(df$white_per) # 2.64 Normal skewness
kurtosis(df$white_per) # 7.034 Highly Kutosis
#F Hispanic Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$hispanic_per, df$`SAT Math Avg. Score`,xlab = "Hispanic %", ylab = "SAT Math AQvg Score")
TEregF = lm(df$`SAT Math Avg. Score` ~ df$hispanic_per, data = df)
abline(TEregF,col="red")
distF = table(df$hispanic_per) # making a frequency table to see if its normal
barplot(distF) 
hist(distF, breaks = 10)
skewness(df$hispanic_per) # 0.1315 Normal skewness
kurtosis(df$hispanic_per) # -1.03371 Normal kurtosis
 #G Asian Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$asian_per, df$`SAT Math Avg. Score`,xlab = "Asian %", ylab = "SAT Math AQvg Score")
TEregG = lm(df$`SAT Math Avg. Score` ~ df$asian_per, data = df)
abline(TEregG,col="red")
distG = table(df$asian_per) # making a frequency table to see if its normal
barplot(distG) 
hist(distG, breaks = 10)
skewness(df$asian_per) # 2.47 Normal skewness
kurtosis(df$asian_per) # 6.57 Abnormal Kutosis
#H ELL Percent Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$ell_percent, df$`SAT Math Avg. Score`,xlab = "Number of English Learners %", ylab = "SAT Math AQvg Score")
TEregH = lm(df$`SAT Math Avg. Score` ~ df$ell_percent, data = df)
abline(TEregH,col="red")
distH = table(df$ell_percent) # making a frequency table to see if its normal
barplot(distH) 
hist(distH, breaks = 10)
skewness(df$ell_percent) # 2.95 Normal skewness
kurtosis(df$ell_percent) # 8.34 Abnormal Kutosis
#I FRL Percent  Scatter Plot + Bar Plot + Skewness + Kutosis
plot(df$frl_percent, df$`SAT Math Avg. Score`,xlab = "Free and Reduced Lunch %", ylab = "SAT Math AQvg Score")
TEregI = lm(df$`SAT Math Avg. Score` ~ df$frl_percent, data = df)
abline(TEregI,col="red")
distI = table(df$frl_percent) # making a frequency table to see if its normal
barplot(distI) 
hist(distI, breaks = 10)
skewness(df$frl_percent) # -1.00 Normal skewness
kurtosis(df$frl_percent) # 0.88 Normal Kutosis
 #5: Check for missing values. DId not see any mising values
summary(df)
#6 Checking for outliers
MLR = cbind(total_enrollment, male_per, female_per, black_per, white_per, hispanic_per, asian_per, ell_percent, frl_percent)
mahal = mahalanobis(MLR,
                    colMeans(MLR, na.rm = TRUE),
                    cov(MLR, use = "pairwise.complete.obs"))
mahal
cutoff = qchisq(1-0.001,ncol(MLR)) # detrming the cutoff point
print(cutoff)
summary(mahal < cutoff) # Determining the number of outliers. 16 outliers. WE are not taking out outliers due to two #reasons:
#1) Our data is an average score of all schools so therefore, we can not identify them on a unique basis
#2) We need to be concerned about outliers that are possible errors. Here scoring a perfect 800 is possible and the max SAT math score in our data is 735.
noout = subset(MLR, mahal < cutoff) # Values which are not outliers
nooutd = as.data.frame(t(noout))
str(noout)
# 7 Multicolinearity check./ As per Raj's analysis all the values are less than 0.90
#MLR
olsreg2Math = lm(`SAT Math Avg. Score` ~ MLR)
summary(olsreg2Math) # Only ELL & FRL are significantly different than 0 so include them in the model. The other independent variables are not significatly different from 0 therefore holding all things constant these independant variables do not impact the dependant variable 
#8 Linearity check. 
plot(olsreg2Math,2) # The plot shows that the standardized errors are normally distributed. 
plot(olsreg2Math,1) # Plot to check for homogenity and homoscedasticity. To me it seems homogenous spread above the line is almost equal to spread below the line but I dont think its homoscedastic as its not uniform along the x axis. Katge Valentine Prof mentioned use bootstraping, I have no clue what that is. 
anova(olsreg2Math)# Since P value is less than 0.05 therefore we have a joint  significant result
confint(olsreg2Math, level = 0.95) # The ones that dont contain the 0 in the intervaql are significant which is confirmed by our other results
yhat = fitted(olsreg2Math) # Predicted values for the dependent variable.
summary(yhat)
# Regression Residuals
ehat = resid(olsreg2Math)
summary(ehat) # SUm of residuals on average is 0




















```
